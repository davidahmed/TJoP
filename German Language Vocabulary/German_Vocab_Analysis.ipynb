{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION I\n",
    "\n",
    "> **Try to find the most common words in the German Language, and as a result come up with a better frequency dictionary or your personal vocabulary list.**\n",
    "\n",
    "In almost every natural language, there are some patterns, be it vocabulary, or grammatical constructs, or even the letters, that are present in a disproportionate amount. In other words, some words and patterns appear more often than others. For an enhanced approach to memorizing a high-return vocabulary, one might find these helpful. After all, why learn an arcane word early on when you can memorize frequent ones?\n",
    "\n",
    "As a Python programmer, you'll find it even more joyful and rewarding to go through this analysis on your own. If you're interested, you may read more [here](https://github.com/davidahmed/TJoP) as to why this notebook exists.\n",
    "\n",
    "In this section, you will encounter:\n",
    "* basic text-file reading\n",
    "* mapping lists (aka functors)\n",
    "* very frequent functional operations such as map, reduce, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The corpus was downloaded from here: \n",
    "#  https://wortschatz.uni-leipzig.de/en/download/\n",
    "\n",
    "CORPUS_PATH = './corpus/deu_news_2015_300K-{corpus_type}.txt'\n",
    "\n",
    "CORPUS_WORDS_PATH = CORPUS_PATH.format(corpus_type='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t!\t!\t6010\r\n",
      "2\t\"\t\"\t57894\r\n",
      "4\t$\t$\t87\r\n",
      "5\t%\t%\t229\r\n",
      "6\t&\t&\t1265\r\n",
      "7\t'\t'\t1477\r\n",
      "8\t(\t(\t18870\r\n",
      "9\t)\t)\t18740\r\n",
      "10\t*\t*\t573\r\n",
      "11\t+\t+\t789\r\n"
     ]
    }
   ],
   "source": [
    "# Any command prefixed with an exclamation mark (!) is \n",
    "#  interpreted as a shell command.\n",
    "\n",
    "!head {CORPUS_WORDS_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "with open(CORPUS_WORDS_PATH) as r:\n",
    "    for line in r:\n",
    "        line = line.split()\n",
    "        words.append((line[2], int(line[-1])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(filter(lambda _: len(_[0]) != 1, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(map(lambda _: _[-1], words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words cover 13% of the corpus!\n",
      "Top 100 words cover 36% of the corpus!\n",
      "Top 200 words cover 42% of the corpus!\n",
      "Top 500 words cover 50% of the corpus!\n",
      "Top 1000 words cover 57% of the corpus!\n",
      "Top 2000 words cover 63% of the corpus!\n",
      "Top 5000 words cover 72% of the corpus!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Ns = [10, 100, 200, 500, 1000, 2000, 5000]\n",
    "\n",
    "for n in Ns:\n",
    "    prozent = sum(map(lambda _: _[1], words[:n]))/total_count*100\n",
    "    print(\"Top {n} words cover {prozent}% of the corpus!\".format(n=n, \n",
    "                                                                prozent=math.floor(prozent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "At this point, we are counting the 'raw/textual' word-frequency. In German, like almost in any other language, the same lemma has various lexemes. For instance, the word der, die, das are lexemes used as the English language article *the* in the German language. [Lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)), thus, is the canonical form of lexemes; whereas a [lexeme](https://en.wikipedia.org/wiki/Lexeme) is just the smallest and independent unit of 'meaning'.\n",
    "\n",
    "In other words, if you were to look up the German words *kommt*, *kommen*, *komme*, etc., they would all be listed under the same lemma (kommen) in the dictionary. In order to have a mapping of our lexemes in the `words`, we would need some Natural Language Processing. Assuming it's a blackbox which tells us the mapping, so we may finish the task at hand. I personally would rather know the lemma and it's most common lexemes, than not know it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 799 ms, total: 1min 16s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# WARNING: I'm finding the lemmas for first 10_000 words only.\n",
    "#   I'm sure there are faster ways to do this rather than \n",
    "#   creating nlp objects for each word. But I'm happy so far.\n",
    "\n",
    "limit = 10_000\n",
    "lemmas = list(map(lambda _: (nlp(_[0])[0].lemma_, _[-1]), words[:10_000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lemmas = {}\n",
    "\n",
    "for lemma in lemmas:\n",
    "    if lemma[0] in unique_lemmas:\n",
    "        unique_lemmas[lemma[0]] += lemma[-1]\n",
    "    else:\n",
    "        unique_lemmas[lemma[0]] = lemma[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lemmas = list(unique_lemmas.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 lemmas cover 20% of the corpus!\n",
      "Top 100 lemmas cover 43% of the corpus!\n",
      "Top 200 lemmas cover 49% of the corpus!\n",
      "Top 500 lemmas cover 57% of the corpus!\n",
      "Top 1000 lemmas cover 63% of the corpus!\n",
      "Top 2000 lemmas cover 69% of the corpus!\n",
      "Top 5000 lemmas cover 76% of the corpus!\n"
     ]
    }
   ],
   "source": [
    "Ns = [10, 100, 200, 500, 1000, 2000, 5000]\n",
    "\n",
    "for n in Ns:\n",
    "    prozent = sum(map(lambda _: _[1], unique_lemmas[:n]))/total_count*100\n",
    "    print(\"Top {n} lemmas cover {prozent}% of the corpus!\".format(n=n, \n",
    "                                                                prozent=math.floor(prozent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "We notice that the coverage goes up after we reduce the lexemes to their canonical form (i.e. lemmas). Note that this step is called **lemmatization**. Of course, this is all a coarse approximation, but nonetheless, could be helpful to 'hacking' vocabulary learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Explorations:\n",
    "\n",
    "> 1. Try coming up with the most common lexemes. For instance, try finding which form (i.e. conjugation) of a verb is most common.\n",
    "\n",
    "> 2. Try a different genre/corpus/time. Do you notice a difference in the distribution of most common words? TIP: You may be able to get txt of old books (genres) from [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "> 3. Can you come up with a better (by your own subjective judgement) frequency dictionary than [these](https://www.routledge.com/Routledge-Frequency-Dictionaries/book-series/RFD?gclid=EAIaIQobChMI046nt5Si6gIVDhoYCh1pnwAlEAAYASAAEgIGZPD_BwE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II \n",
    "\n",
    "> In this section we will get into n-gram analysis, particularly, bigrams.\n",
    "\n",
    "You'll encounter: \n",
    "* some data cleaning, also referred to as **data wrangling**\n",
    "* regex operations\n",
    "* Python Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t00.48 Uhr: Und sie liegt nur 48 Kilometer entfernt.\r\n",
      "2\t00.56 Uhr: Der SRF-Moderator nützt die Nationalhymnen, um sich in Position zu bringen und übernimmt nach den Landesliedern als Kommentator.\r\n",
      "3\t007: Spectre-Weltpremiere in London Sogar die Royals kamen, um sich die Weltpremiere des neune James Bond Films..\r\n",
      "4\t01.00 Uhr - Am Dienstag geht das Tor zu an der ungarisch-serbischen Grenze.\r\n",
      "5\t01.05.2015 – 14:12 Fernsehen München (ots) - Das Thema: Deutschlands Löhne - Was ist unsere Arbeit wert?\r\n",
      "6\t01.07.15 – 01:31 min Mediathek Möglicher Grexit Kurzfristige Versorgung mit Drachmen ist nicht möglich \"Die Griechen leben seit Jahren über ihre Verhältnisse\", so Sinn.\r\n",
      "7\t01.07.2015 – 14:46 Uslar (ots) - USLAR (zi.) Am Sonntag, 28.06.2015, gegen 01.55 Uhr, wurde in der Langen Straße, in Höhe der Stadtschänke, ein 49-jähriger Taxifahrer von einer bisher unbekannten Person unvermittelt angegriffen und ins Gesicht geschlagen.\r\n",
      "8\t01.09.2015 08:00 Uhr Wolfgang Stieler vorlesen Mit Genmodifikationen können solche Bierhefe-Zellen nicht nur Alkohol, sondern auch ein Opioid produzieren.\r\n",
      "9\t01.09.2015 14:50 Uhr Daniel Berger vorlesen Ab dem heutigen 1. September pausiert der Chrome-Browser bestimmte Flash-Inhalte automatisch.\r\n",
      "10\t01.10.2015 12:36 Uhr Rainald Menge-Sonnentag vorlesen Mit dem Konnektor für die BI-Suite Spotfire zum Open-Source-Framework Apache Spark erweitert Tibco seine Verbindung in die Open-Source-Welt für Big-Data.\r\n"
     ]
    }
   ],
   "source": [
    "! head {CORPUS_SENTENCES_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./corpus/deu_news_2015_300K-sentences.txt: text/plain; charset=utf-8\r\n"
     ]
    }
   ],
   "source": [
    "! file -I {CORPUS_SENTENCES_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "CORPUS_PATH = './corpus/deu_news_2015_300K-{corpus_type}.txt'\n",
    "\n",
    "CORPUS_SENTENCES_PATH = CORPUS_PATH.format(corpus_type='sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open(CORPUS_SENTENCES_PATH, 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sanitizeTimestamp(sentence):\n",
    "    match = re.search(r'\\sUhr[\\:]*\\s', sentence)\n",
    "    if match:\n",
    "        return sentence[match.end():].strip()\n",
    "    else: \n",
    "        return sentence.split('\\t')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(lambda _: sanitizeTimestamp(_), sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sentences.split(' ')\n",
    "bigrams = []\n",
    "\n",
    "for _, word in enumerate(words[:-1]):\n",
    "    bigrams.append(' '.join(words[_:_+2]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 bigrams in the German language:\n",
      "\n",
      "in der\n",
      "in den\n",
      "für die\n",
      "mit dem\n",
      "auf die\n",
      "und die\n",
      "in die\n",
      "bei der\n",
      "mit der\n",
      "auf der\n",
      "auf dem\n",
      "für den\n",
      "an der\n",
      "von der\n",
      "mit einem\n",
      "auf den\n",
      "dass die\n",
      "aus dem\n",
      "sich die\n",
      "gibt es\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "\n",
    "print('Top {N} bigrams in the German language:\\n'.format(N=N))\n",
    "\n",
    "for bigram in bigrams.most_common(N):\n",
    "    print(bigram[0], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "If you're new to the German language, now you shouldn't be surprised to see the bigrams like used so frequently in written text.\n",
    "\n",
    "Also if you noticed, I didn't attempt to calculate the percentage (*prozent* in German) for the bigrams. I did so because percentage really doesn't make sense here- precisely because there are so many ways to calculate it, and also, the total number of possible bigrams would be a huge number (size_of_vocab^2). Relative frequency (or probability weights or the counts as I did here) would be all rather a better approach and thinking models.\n",
    "\n",
    "I personally found that looking at and learning n-grams (i.e. bigrams, trigrams, quadrams, etc.) is usually a very interesting alternate way to learning the vocabulary of a language. For instance, *gibt es*, 20-th most common bigram in our corpus could actually be quite handy. It roughly means 'is there'; a handy phrase to ask in situations like- \"(Do you sell/have)/(Is there) mayonnaise?\". You'd say, \"Gibt es Mayonnaise?\". Or *Gibt es hier eine Zukunft?*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Explorations:\n",
    "\n",
    "> Try exploring other n-grams. You'd find that the higer your n goes, the little do things make sense. Why? \n",
    ">\n",
    "> If you're interested, you might want to look into what Claude Shannon (a great computer scientist that you should know of) showed how a generative model of a language \n",
    "> can be created using n-gram analysis alone. ([Here's](https://web.stanford.edu/~jurafsky/slp3/3.pdf) a good reading)\n",
    "\n",
    "> If you remember, I used `spacy` library above to do **lemmatization**. You can actually do something very interesting- explore what are the most common sentence structures in the German language!\n",
    "\n",
    "> If you're a language enthusiast, you might find exploring other languages. A language like Arabic could be very rewarding if you've never encoundered it in NLP context before- it's (MSA/ Classical Arabic) morphology and templatic pattern might hook you for a lifetime. But of course, all languages are just beautiful and mesmerizing and divine. Aren't they?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
